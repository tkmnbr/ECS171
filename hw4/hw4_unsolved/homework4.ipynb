{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'day': array(['Thursday', 'Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday'],\n",
      "      dtype=object), 'quarter': array(['Quarter1', 'Quarter2', 'Quarter3', 'Quarter4', 'Quarter5'],\n",
      "      dtype=object), 'department': array(['sweing', 'finishing ', 'finishing'], dtype=object), 'team': array([ 8,  1, 11, 12,  6,  7,  2,  3,  9, 10,  5,  4])}\n",
      "Index(['wip'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('garments_worker_productivity.csv')\n",
    "\n",
    "# Drop the date column\n",
    "if 'date' in data.columns:\n",
    "    data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# For each of the categorical attributes, print all the unique elements.\n",
    "categorical_columns = ['day', 'quarter', 'department', 'team']\n",
    "unique_values = {col: data[col].unique() for col in categorical_columns}\n",
    "print(unique_values)\n",
    "# From the unique values output, we can see that the there is a duplicated values in department column, which is 'finishing'\n",
    "# since one of them has extra space after the word.\n",
    "# We need to clean the data\n",
    "\n",
    "# Data cleaning for categorical attributes\n",
    "data['day'] = data['day'].str.strip().str.lower()\n",
    "data['quarter'] = data['quarter'].str.strip().str.lower()\n",
    "data['department'] = data['department'].str.strip().str.lower()\n",
    "\n",
    "unique_values = {col: data[col].unique() for col in categorical_columns}\n",
    "\n",
    "# Create another column named satisfied that records the productivity performance.\n",
    "data['satisfied'] = (data['actual_productivity'] >= data['targeted_productivity']).astype(int)\n",
    "\n",
    "# Drop the columns actual_productivity and targeted_productivity\n",
    "data.drop(columns=['actual_productivity', 'targeted_productivity'], inplace=True)\n",
    "\n",
    "# Find and print which columns/attributes have empty values\n",
    "missing_columns = data.columns[data.isnull().any()]\n",
    "print(missing_columns) # wip has empty value\n",
    "\n",
    "# Fill the empty values with 0\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score  support\n",
      "0              0.434783  0.158730  0.232558   63.000\n",
      "1              0.755760  0.926554  0.832487  177.000\n",
      "accuracy       0.725000  0.725000  0.725000    0.725\n",
      "macro avg      0.595271  0.542642  0.532523  240.000\n",
      "weighted avg   0.671504  0.725000  0.675006  240.000\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Exercise 2.1\n",
    "# For each of the categorical attributes, encode the set of categories using integers 0, ... n - 1\n",
    "label_encoders = {}\n",
    "for col in ['day', 'quarter', 'department', 'team']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Split the data into training and testing sets with a ratio of 80:20\n",
    "X = data.drop(columns=['satisfied'])\n",
    "y = data['satisfied']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Exercise 2.2\n",
    "# Use the categorical attributes only\n",
    "categorical_features = ['day', 'quarter', 'department', 'team']\n",
    "\n",
    "# please build a Categorical Na√Øve Bayes classifier that predicts the column satisfied. \n",
    "X_train_cat = X_train[categorical_features]\n",
    "X_test_cat = X_test[categorical_features]\n",
    "\n",
    "nb_model = CategoricalNB()\n",
    "nb_model.fit(X_train_cat, y_train)\n",
    "\n",
    "y_pred = nb_model.predict(X_test_cat)\n",
    "\n",
    "# Report the testing result using classification_report\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "print(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score     support\n",
      "0              0.689655  0.317460  0.434783   63.000000\n",
      "1              0.796209  0.949153  0.865979  177.000000\n",
      "accuracy       0.783333  0.783333  0.783333    0.783333\n",
      "macro avg      0.742932  0.633306  0.650381  240.000000\n",
      "weighted avg   0.768238  0.783333  0.752790  240.000000\n",
      "              precision    recall  f1-score   support\n",
      "0              0.600000  0.285714  0.387097   63.0000\n",
      "1              0.785714  0.932203  0.852713  177.0000\n",
      "accuracy       0.762500  0.762500  0.762500    0.7625\n",
      "macro avg      0.692857  0.608959  0.619905  240.0000\n",
      "weighted avg   0.736964  0.762500  0.730489  240.0000\n",
      "satisfied\n",
      "1    698\n",
      "0    259\n",
      "Name: count, dtype: int64\n",
      "satisfied\n",
      "1    698\n",
      "0    698\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score     support\n",
      "0              0.431373  0.698413  0.533333   63.000000\n",
      "1              0.862319  0.672316  0.755556  177.000000\n",
      "accuracy       0.679167  0.679167  0.679167    0.679167\n",
      "macro avg      0.646846  0.685365  0.644444  240.000000\n",
      "weighted avg   0.749195  0.679167  0.697222  240.000000\n",
      "              precision    recall  f1-score     support\n",
      "0              0.382716  0.492063  0.430556   63.000000\n",
      "1              0.798742  0.717514  0.755952  177.000000\n",
      "accuracy       0.658333  0.658333  0.658333    0.658333\n",
      "macro avg      0.590729  0.604789  0.593254  240.000000\n",
      "weighted avg   0.689535  0.658333  0.670536  240.000000\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Exercise 3.1\n",
    "# For each of the categorical attributes, encode them with one-hot encoding.\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "X_categorical_encoded = encoder.fit_transform(data[['day', 'quarter', 'department', 'team']])\n",
    "\n",
    "encoded_feature_names = encoder.get_feature_names_out(['day', 'quarter', 'department', 'team'])\n",
    "X_categorical_df = pd.DataFrame(X_categorical_encoded)\n",
    "\n",
    "X_numerical = data.drop(columns=['day', 'quarter', 'department', 'team', 'satisfied'])\n",
    "X_encoded = pd.concat([X_categorical_df, X_numerical.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Split the data into training and testing sets with a ratio of 80:20.\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Exercise 3.2\n",
    "# Using all the attributes we have, please build a SVM that predicts the column satisfied.\n",
    "X_train_svm.columns = X_train_svm.columns.astype(str)\n",
    "X_test_svm.columns = X_test_svm.columns.astype(str)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_svm_scaled = scaler.fit_transform(X_train_svm)\n",
    "X_test_svm_scaled = scaler.fit_transform(X_test_svm)\n",
    "\n",
    "# Build one SVM with a linear kernel.\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train_svm_scaled, y_train_svm)\n",
    "y_pred_linear = svm_linear.predict(X_test_svm_scaled)\n",
    "\n",
    "# Build another SVM with an RBF kernel.\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train_svm_scaled, y_train_svm)\n",
    "y_pred_rbf = svm_rbf.predict(X_test_svm_scaled)\n",
    "\n",
    "# Report the testing results of both models using classification_report.\n",
    "report_linear = classification_report(y_test_svm, y_pred_linear, output_dict=True)\n",
    "report_rbf = classification_report(y_test_svm, y_pred_rbf, output_dict=True)\n",
    "\n",
    "report_linear_df = pd.DataFrame(report_linear).transpose()\n",
    "report_rbf_df = pd.DataFrame(report_rbf).transpose()\n",
    "\n",
    "print(report_linear_df)\n",
    "\n",
    "print(report_rbf_df)\n",
    "\n",
    "\n",
    "# Exercise 3.3\n",
    "\n",
    "# For the column satisfied in the training set, print the frequency of each class.\n",
    "class_counts_before = y_train_svm.value_counts()\n",
    "print(class_counts_before)\n",
    "\n",
    "# Oversample the training data.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_svm_resampled, y_train_svm_resampled = smote.fit_resample(X_train_svm_scaled, y_train_svm)\n",
    "\n",
    "# For the column satisfied in the oversampled data, print the frequency of each class again.\n",
    "class_counts_after = pd.Series(y_train_svm_resampled).value_counts()\n",
    "print(class_counts_after)\n",
    "\n",
    "# Re-build the two SVMs using the same settings as in Exercise 3.2, but use the oversampled training data instead.\n",
    "# Since we used scaled data, we do not need to scale again\n",
    "svm_linear_resampled = SVC(kernel='linear', random_state=42)\n",
    "svm_linear_resampled.fit(X_train_svm_resampled, y_train_svm_resampled)\n",
    "y_pred_linear_resampled = svm_linear_resampled.predict(X_test_svm_scaled)\n",
    "\n",
    "svm_rbf_resampled = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf_resampled.fit(X_train_svm_resampled, y_train_svm_resampled)\n",
    "y_pred_rbf_resampled = svm_rbf_resampled.predict(X_test_svm_scaled)\n",
    "\n",
    "# Report the testing results using classification_report.\n",
    "report_linear_resampled = classification_report(y_test_svm, y_pred_linear_resampled, output_dict=True)\n",
    "report_rbf_resampled = classification_report(y_test_svm, y_pred_rbf_resampled, output_dict=True)\n",
    "\n",
    "report_linear_resampled_df = pd.DataFrame(report_linear_resampled).transpose()\n",
    "report_rbf_resampled_df = pd.DataFrame(report_rbf_resampled).transpose()\n",
    "\n",
    "print(report_linear_resampled_df)\n",
    "print(report_rbf_resampled_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
