{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "# 1.1\n",
    "# As the classes are categorical, use one-hot encoding to represent the set of classes. \n",
    "# You will find this useful when developing the output layer of the neural network.\n",
    "# Note that the dataset doesn't include data points from all grading levels, \n",
    "# but you may still need to create the one-hot encoding by 10 cols.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "data = pd.read_csv('winequality-white.csv')\n",
    "\n",
    "encoder = OneHotEncoder(categories=[range(1, 11)], sparse=False)\n",
    "encoded_labels = encoder.fit_transform(data[['quality']])\n",
    "encoded_labels_df = pd.DataFrame(encoded_labels, columns=[f'quality_{i}' for i in range(1, 11)])\n",
    "data = pd.concat([data.drop('quality', axis=1), encoded_labels_df], axis=1)\n",
    "\n",
    "# 1.2\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data.iloc[:, :-10])\n",
    "scaled_data_df = pd.DataFrame(scaled_data, columns=data.columns[:-10])\n",
    "final_data = pd.concat([scaled_data_df, encoded_labels_df], axis=1)\n",
    "\n",
    "final_data.to_csv('preprocessed_winequality_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">234</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)             │           \u001b[38;5;34m276\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)             │           \u001b[38;5;34m408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │           \u001b[38;5;34m234\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m140\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,058</span> (4.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,058\u001b[0m (4.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,058</span> (4.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,058\u001b[0m (4.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1743 - loss: 0.7020 - val_accuracy: 0.1918 - val_loss: 0.1465\n",
      "Epoch 2/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1792 - loss: 0.3896 - val_accuracy: 0.1918 - val_loss: 0.1444\n",
      "Epoch 3/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - accuracy: 0.1912 - loss: 0.3865 - val_accuracy: 0.1918 - val_loss: 0.1437\n",
      "Epoch 4/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 0.1799 - loss: 0.3933 - val_accuracy: 0.1918 - val_loss: 0.1439\n",
      "Epoch 5/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - accuracy: 0.1775 - loss: 0.3942 - val_accuracy: 0.1918 - val_loss: 0.1425\n",
      "Epoch 6/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - accuracy: 0.1901 - loss: 0.3864 - val_accuracy: 0.1918 - val_loss: 0.1431\n",
      "Epoch 7/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - accuracy: 0.1814 - loss: 0.3916 - val_accuracy: 0.1918 - val_loss: 0.1420\n",
      "Epoch 8/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - accuracy: 0.1850 - loss: 0.3874 - val_accuracy: 0.1918 - val_loss: 0.1424\n",
      "Epoch 9/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - accuracy: 0.1739 - loss: 0.3910 - val_accuracy: 0.1918 - val_loss: 0.1449\n",
      "Epoch 10/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1759 - loss: 0.3992 - val_accuracy: 0.1918 - val_loss: 0.1455\n",
      "Epoch 11/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.1850 - loss: 0.3858 - val_accuracy: 0.1918 - val_loss: 0.1430\n",
      "Epoch 12/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.1731 - loss: 0.3926 - val_accuracy: 0.1918 - val_loss: 0.1456\n",
      "Epoch 13/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1738 - loss: 0.3887 - val_accuracy: 0.1918 - val_loss: 0.1432\n",
      "Epoch 14/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - accuracy: 0.1747 - loss: 0.3931 - val_accuracy: 0.1918 - val_loss: 0.1438\n",
      "Epoch 15/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1833 - loss: 0.3799 - val_accuracy: 0.1918 - val_loss: 0.1438\n",
      "Epoch 16/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - accuracy: 0.1695 - loss: 0.3899 - val_accuracy: 0.1918 - val_loss: 0.1441\n",
      "Epoch 17/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - accuracy: 0.1788 - loss: 0.3887 - val_accuracy: 0.1918 - val_loss: 0.1443\n",
      "Epoch 18/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.1841 - loss: 0.3838 - val_accuracy: 0.1918 - val_loss: 0.1445\n",
      "Epoch 19/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.1711 - loss: 0.3916 - val_accuracy: 0.1918 - val_loss: 0.1431\n",
      "Epoch 20/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1647 - loss: 0.3930 - val_accuracy: 0.1918 - val_loss: 0.1471\n",
      "Epoch 21/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - accuracy: 0.1679 - loss: 0.3944 - val_accuracy: 0.1918 - val_loss: 0.1452\n",
      "Epoch 22/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.1712 - loss: 0.3873 - val_accuracy: 0.1918 - val_loss: 0.1448\n",
      "Epoch 23/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1745 - loss: 0.3941 - val_accuracy: 0.1918 - val_loss: 0.1421\n",
      "Epoch 24/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - accuracy: 0.1749 - loss: 0.3865 - val_accuracy: 0.1918 - val_loss: 0.1458\n",
      "Epoch 25/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - accuracy: 0.1777 - loss: 0.3882 - val_accuracy: 0.1918 - val_loss: 0.1428\n",
      "Epoch 26/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 0.1815 - loss: 0.3932 - val_accuracy: 0.1918 - val_loss: 0.1426\n",
      "Epoch 27/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.1782 - loss: 0.3902 - val_accuracy: 0.1918 - val_loss: 0.1429\n",
      "Epoch 28/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 0.1734 - loss: 0.3951 - val_accuracy: 0.1918 - val_loss: 0.1450\n",
      "Epoch 29/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 0.1774 - loss: 0.3872 - val_accuracy: 0.1918 - val_loss: 0.1455\n",
      "Epoch 30/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.1804 - loss: 0.3839 - val_accuracy: 0.1918 - val_loss: 0.1456\n",
      "Epoch 31/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - accuracy: 0.1720 - loss: 0.3930 - val_accuracy: 0.1918 - val_loss: 0.1449\n",
      "Epoch 32/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1807 - loss: 0.3924 - val_accuracy: 0.1918 - val_loss: 0.1422\n",
      "Epoch 33/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.1852 - loss: 0.3889 - val_accuracy: 0.1918 - val_loss: 0.1421\n",
      "Epoch 34/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1754 - loss: 0.3888 - val_accuracy: 0.1918 - val_loss: 0.1430\n",
      "Epoch 35/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - accuracy: 0.1823 - loss: 0.3860 - val_accuracy: 0.1918 - val_loss: 0.1426\n",
      "Epoch 36/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - accuracy: 0.1693 - loss: 0.3877 - val_accuracy: 0.1918 - val_loss: 0.1450\n",
      "Epoch 37/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - accuracy: 0.1895 - loss: 0.3891 - val_accuracy: 0.1918 - val_loss: 0.1419\n",
      "Epoch 38/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.1699 - loss: 0.3945 - val_accuracy: 0.1918 - val_loss: 0.1439\n",
      "Epoch 39/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 0.1757 - loss: 0.3867 - val_accuracy: 0.1918 - val_loss: 0.1437\n",
      "Epoch 40/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - accuracy: 0.1732 - loss: 0.3916 - val_accuracy: 0.1918 - val_loss: 0.1433\n",
      "Epoch 41/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - accuracy: 0.1783 - loss: 0.3868 - val_accuracy: 0.1918 - val_loss: 0.1451\n",
      "Epoch 42/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1860 - loss: 0.3907 - val_accuracy: 0.1918 - val_loss: 0.1445\n",
      "Epoch 43/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - accuracy: 0.1739 - loss: 0.3940 - val_accuracy: 0.1918 - val_loss: 0.1447\n",
      "Epoch 44/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.1778 - loss: 0.3913 - val_accuracy: 0.1918 - val_loss: 0.1451\n",
      "Epoch 45/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.1678 - loss: 0.3957 - val_accuracy: 0.1918 - val_loss: 0.1441\n",
      "Epoch 46/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.1880 - loss: 0.3850 - val_accuracy: 0.1918 - val_loss: 0.1440\n",
      "Epoch 47/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.1690 - loss: 0.3897 - val_accuracy: 0.1918 - val_loss: 0.1453\n",
      "Epoch 48/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - accuracy: 0.1840 - loss: 0.3880 - val_accuracy: 0.1918 - val_loss: 0.1439\n",
      "Epoch 49/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - accuracy: 0.1764 - loss: 0.3890 - val_accuracy: 0.1918 - val_loss: 0.1447\n",
      "Epoch 50/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.1827 - loss: 0.3916 - val_accuracy: 0.1918 - val_loss: 0.1434\n",
      "Epoch 51/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1728 - loss: 0.3958 - val_accuracy: 0.1918 - val_loss: 0.1459\n",
      "Epoch 52/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - accuracy: 0.1797 - loss: 0.3975 - val_accuracy: 0.1918 - val_loss: 0.1454\n",
      "Epoch 53/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.1742 - loss: 0.3884 - val_accuracy: 0.1918 - val_loss: 0.1438\n",
      "Epoch 54/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - accuracy: 0.1665 - loss: 0.3942 - val_accuracy: 0.1918 - val_loss: 0.1438\n",
      "Epoch 55/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.1763 - loss: 0.3939 - val_accuracy: 0.1918 - val_loss: 0.1443\n",
      "Epoch 56/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.1794 - loss: 0.3896 - val_accuracy: 0.1918 - val_loss: 0.1400\n",
      "Epoch 57/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.1750 - loss: 0.3909 - val_accuracy: 0.1918 - val_loss: 0.1432\n",
      "Epoch 58/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1826 - loss: 0.3835 - val_accuracy: 0.1918 - val_loss: 0.1442\n",
      "Epoch 59/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 0.1793 - loss: 0.3867 - val_accuracy: 0.1918 - val_loss: 0.1443\n",
      "Epoch 60/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - accuracy: 0.1758 - loss: 0.3885 - val_accuracy: 0.1918 - val_loss: 0.1422\n",
      "Epoch 61/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1782 - loss: 0.3944 - val_accuracy: 0.1918 - val_loss: 0.1440\n",
      "Epoch 62/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1877 - loss: 0.3827 - val_accuracy: 0.1918 - val_loss: 0.1435\n",
      "Epoch 63/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1748 - loss: 0.3883 - val_accuracy: 0.1918 - val_loss: 0.1470\n",
      "Epoch 64/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.1639 - loss: 0.3957 - val_accuracy: 0.1918 - val_loss: 0.1435\n",
      "Epoch 65/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - accuracy: 0.1839 - loss: 0.3872 - val_accuracy: 0.1918 - val_loss: 0.1429\n",
      "Epoch 66/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.1693 - loss: 0.3911 - val_accuracy: 0.1918 - val_loss: 0.1452\n",
      "Epoch 67/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.1690 - loss: 0.3928 - val_accuracy: 0.1918 - val_loss: 0.1446\n",
      "Epoch 68/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.1870 - loss: 0.3844 - val_accuracy: 0.1918 - val_loss: 0.1425\n",
      "Epoch 69/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - accuracy: 0.1828 - loss: 0.3853 - val_accuracy: 0.1918 - val_loss: 0.1434\n",
      "Epoch 70/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1775 - loss: 0.3944 - val_accuracy: 0.1918 - val_loss: 0.1449\n",
      "Epoch 71/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - accuracy: 0.1726 - loss: 0.3871 - val_accuracy: 0.1918 - val_loss: 0.1459\n",
      "Epoch 72/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.1746 - loss: 0.3846 - val_accuracy: 0.1918 - val_loss: 0.1444\n",
      "Epoch 73/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.1719 - loss: 0.3897 - val_accuracy: 0.1918 - val_loss: 0.1445\n",
      "Epoch 74/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.1759 - loss: 0.3848 - val_accuracy: 0.1918 - val_loss: 0.1414\n",
      "Epoch 75/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1749 - loss: 0.3891 - val_accuracy: 0.1918 - val_loss: 0.1442\n",
      "Epoch 76/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - accuracy: 0.1724 - loss: 0.3876 - val_accuracy: 0.1918 - val_loss: 0.1452\n",
      "Epoch 77/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - accuracy: 0.1828 - loss: 0.3870 - val_accuracy: 0.1918 - val_loss: 0.1446\n",
      "Epoch 78/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.1728 - loss: 0.3808 - val_accuracy: 0.1918 - val_loss: 0.1439\n",
      "Epoch 79/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.1826 - loss: 0.3901 - val_accuracy: 0.1918 - val_loss: 0.1425\n",
      "Epoch 80/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.1721 - loss: 0.3954 - val_accuracy: 0.1918 - val_loss: 0.1446\n",
      "Epoch 81/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1849 - loss: 0.3783 - val_accuracy: 0.1918 - val_loss: 0.1420\n",
      "Epoch 82/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - accuracy: 0.1770 - loss: 0.3923 - val_accuracy: 0.1918 - val_loss: 0.1443\n",
      "Epoch 83/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - accuracy: 0.1747 - loss: 0.3847 - val_accuracy: 0.1918 - val_loss: 0.1451\n",
      "Epoch 84/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1835 - loss: 0.3876 - val_accuracy: 0.1918 - val_loss: 0.1436\n",
      "Epoch 85/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.1758 - loss: 0.3863 - val_accuracy: 0.1918 - val_loss: 0.1428\n",
      "Epoch 86/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - accuracy: 0.1727 - loss: 0.3901 - val_accuracy: 0.1918 - val_loss: 0.1430\n",
      "Epoch 87/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.1658 - loss: 0.3893 - val_accuracy: 0.1918 - val_loss: 0.1460\n",
      "Epoch 88/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.1768 - loss: 0.3930 - val_accuracy: 0.1918 - val_loss: 0.1465\n",
      "Epoch 89/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.1764 - loss: 0.3944 - val_accuracy: 0.1918 - val_loss: 0.1442\n",
      "Epoch 90/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.1750 - loss: 0.3842 - val_accuracy: 0.1918 - val_loss: 0.1424\n",
      "Epoch 91/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.1838 - loss: 0.3820 - val_accuracy: 0.1918 - val_loss: 0.1420\n",
      "Epoch 92/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - accuracy: 0.1742 - loss: 0.3883 - val_accuracy: 0.1918 - val_loss: 0.1449\n",
      "Epoch 93/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - accuracy: 0.1756 - loss: 0.3900 - val_accuracy: 0.1918 - val_loss: 0.1439\n",
      "Epoch 94/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1805 - loss: 0.3868 - val_accuracy: 0.1918 - val_loss: 0.1438\n",
      "Epoch 95/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.1863 - loss: 0.3851 - val_accuracy: 0.1918 - val_loss: 0.1421\n",
      "Epoch 96/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1810 - loss: 0.3855 - val_accuracy: 0.1918 - val_loss: 0.1445\n",
      "Epoch 97/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.1882 - loss: 0.3888 - val_accuracy: 0.1918 - val_loss: 0.1451\n",
      "Epoch 98/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 0.1765 - loss: 0.3940 - val_accuracy: 0.1918 - val_loss: 0.1447\n",
      "Epoch 99/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.1687 - loss: 0.3957 - val_accuracy: 0.1918 - val_loss: 0.1431\n",
      "Epoch 100/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.1812 - loss: 0.3932 - val_accuracy: 0.1918 - val_loss: 0.1421\n",
      "Epoch 101/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1800 - loss: 0.3816 - val_accuracy: 0.1918 - val_loss: 0.1414\n",
      "Epoch 102/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.1820 - loss: 0.3786 - val_accuracy: 0.1918 - val_loss: 0.1442\n",
      "Epoch 103/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1804 - loss: 0.3861 - val_accuracy: 0.1918 - val_loss: 0.1434\n",
      "Epoch 104/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.1771 - loss: 0.3887 - val_accuracy: 0.1918 - val_loss: 0.1416\n",
      "Epoch 105/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1712 - loss: 0.3850 - val_accuracy: 0.1918 - val_loss: 0.1440\n",
      "Epoch 106/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - accuracy: 0.1805 - loss: 0.3825 - val_accuracy: 0.1918 - val_loss: 0.1450\n",
      "Epoch 107/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1717 - loss: 0.3859 - val_accuracy: 0.1918 - val_loss: 0.1445\n",
      "Epoch 108/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - accuracy: 0.1757 - loss: 0.3879 - val_accuracy: 0.1918 - val_loss: 0.1425\n",
      "Epoch 109/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.1744 - loss: 0.3846 - val_accuracy: 0.1918 - val_loss: 0.1405\n",
      "Epoch 110/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.1912 - loss: 0.3790 - val_accuracy: 0.1918 - val_loss: 0.1427\n",
      "Epoch 111/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.1774 - loss: 0.3890 - val_accuracy: 0.1918 - val_loss: 0.1409\n",
      "Epoch 112/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.1821 - loss: 0.3879 - val_accuracy: 0.1918 - val_loss: 0.1428\n",
      "Epoch 113/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.1782 - loss: 0.3921 - val_accuracy: 0.1918 - val_loss: 0.1420\n",
      "Epoch 114/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.1817 - loss: 0.3803 - val_accuracy: 0.1918 - val_loss: 0.1428\n",
      "Epoch 115/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1788 - loss: 0.3907 - val_accuracy: 0.1918 - val_loss: 0.1451\n",
      "Epoch 116/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.1752 - loss: 0.3828 - val_accuracy: 0.1918 - val_loss: 0.1446\n",
      "Epoch 117/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.1760 - loss: 0.3887 - val_accuracy: 0.1918 - val_loss: 0.1438\n",
      "Epoch 118/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1718 - loss: 0.3874 - val_accuracy: 0.1918 - val_loss: 0.1429\n",
      "Epoch 119/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.1813 - loss: 0.3902 - val_accuracy: 0.1918 - val_loss: 0.1409\n",
      "Epoch 120/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.1799 - loss: 0.3886 - val_accuracy: 0.1918 - val_loss: 0.1451\n",
      "Epoch 121/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.1848 - loss: 0.3868 - val_accuracy: 0.1918 - val_loss: 0.1428\n",
      "Epoch 122/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.1728 - loss: 0.3897 - val_accuracy: 0.1918 - val_loss: 0.1457\n",
      "Epoch 123/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.1743 - loss: 0.3938 - val_accuracy: 0.1918 - val_loss: 0.1449\n",
      "Epoch 124/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.1857 - loss: 0.3805 - val_accuracy: 0.1918 - val_loss: 0.1420\n",
      "Epoch 125/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - accuracy: 0.1777 - loss: 0.3887 - val_accuracy: 0.1918 - val_loss: 0.1425\n",
      "Epoch 126/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.1841 - loss: 0.3847 - val_accuracy: 0.1918 - val_loss: 0.1418\n",
      "Epoch 127/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - accuracy: 0.1707 - loss: 0.3895 - val_accuracy: 0.1918 - val_loss: 0.1450\n",
      "Epoch 128/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1831 - loss: 0.3829 - val_accuracy: 0.1918 - val_loss: 0.1435\n",
      "Epoch 129/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.1749 - loss: 0.3832 - val_accuracy: 0.1918 - val_loss: 0.1445\n",
      "Epoch 130/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.1785 - loss: 0.3915 - val_accuracy: 0.1918 - val_loss: 0.1443\n",
      "Epoch 131/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1767 - loss: 0.3863 - val_accuracy: 0.1918 - val_loss: 0.1413\n",
      "Epoch 132/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - accuracy: 0.1756 - loss: 0.3895 - val_accuracy: 0.1918 - val_loss: 0.1406\n",
      "Epoch 133/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - accuracy: 0.1750 - loss: 0.3920 - val_accuracy: 0.1918 - val_loss: 0.1421\n",
      "Epoch 134/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1722 - loss: 0.3916 - val_accuracy: 0.1918 - val_loss: 0.1448\n",
      "Epoch 135/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.1791 - loss: 0.3847 - val_accuracy: 0.1918 - val_loss: 0.1371\n",
      "Epoch 136/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.1830 - loss: 0.3916 - val_accuracy: 0.1918 - val_loss: 0.1415\n",
      "Epoch 137/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.1800 - loss: 0.3905 - val_accuracy: 0.1918 - val_loss: 0.1424\n",
      "Epoch 138/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.1809 - loss: 0.3849 - val_accuracy: 0.1918 - val_loss: 0.1433\n",
      "Epoch 139/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - accuracy: 0.1706 - loss: 0.3858 - val_accuracy: 0.1918 - val_loss: 0.1428\n",
      "Epoch 140/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.1778 - loss: 0.3856 - val_accuracy: 0.1918 - val_loss: 0.1392\n",
      "Epoch 141/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.1853 - loss: 0.3825 - val_accuracy: 0.1918 - val_loss: 0.1406\n",
      "Epoch 142/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.1779 - loss: 0.3771 - val_accuracy: 0.1918 - val_loss: 0.1396\n",
      "Epoch 143/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.1758 - loss: 0.3858 - val_accuracy: 0.1918 - val_loss: 0.1374\n",
      "Epoch 144/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - accuracy: 0.1786 - loss: 0.3817 - val_accuracy: 0.1918 - val_loss: 0.1382\n",
      "Epoch 145/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.1922 - loss: 0.3853 - val_accuracy: 0.1918 - val_loss: 0.1407\n",
      "Epoch 146/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.1743 - loss: 0.3854 - val_accuracy: 0.1918 - val_loss: 0.1382\n",
      "Epoch 147/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - accuracy: 0.1874 - loss: 0.3853 - val_accuracy: 0.1918 - val_loss: 0.1409\n",
      "Epoch 148/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.1774 - loss: 0.3774 - val_accuracy: 0.1918 - val_loss: 0.1377\n",
      "Epoch 149/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1913 - loss: 0.3790 - val_accuracy: 0.1918 - val_loss: 0.1344\n",
      "Epoch 150/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.1758 - loss: 0.3768 - val_accuracy: 0.1918 - val_loss: 0.1345\n",
      "Epoch 151/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.1750 - loss: 0.3813 - val_accuracy: 0.1918 - val_loss: 0.1382\n",
      "Epoch 152/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 0.1771 - loss: 0.3829 - val_accuracy: 0.1918 - val_loss: 0.1358\n",
      "Epoch 153/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.1719 - loss: 0.3810 - val_accuracy: 0.1918 - val_loss: 0.1395\n",
      "Epoch 154/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - accuracy: 0.1838 - loss: 0.3793 - val_accuracy: 0.1918 - val_loss: 0.1329\n",
      "Epoch 155/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.1789 - loss: 0.3797 - val_accuracy: 0.1918 - val_loss: 0.1350\n",
      "Epoch 156/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step - accuracy: 0.1740 - loss: 0.3846 - val_accuracy: 0.1918 - val_loss: 0.1345\n",
      "Epoch 157/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1852 - loss: 0.3806 - val_accuracy: 0.1918 - val_loss: 0.1322\n",
      "Epoch 158/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - accuracy: 0.1748 - loss: 0.3784 - val_accuracy: 0.1918 - val_loss: 0.1379\n",
      "Epoch 159/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.1804 - loss: 0.3758 - val_accuracy: 0.1918 - val_loss: 0.1361\n",
      "Epoch 160/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - accuracy: 0.1709 - loss: 0.3804 - val_accuracy: 0.1918 - val_loss: 0.1423\n",
      "Epoch 161/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.1826 - loss: 0.3806 - val_accuracy: 0.1918 - val_loss: 0.1372\n",
      "Epoch 162/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.1711 - loss: 0.3834 - val_accuracy: 0.1918 - val_loss: 0.1391\n",
      "Epoch 163/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1745 - loss: 0.3800 - val_accuracy: 0.1918 - val_loss: 0.1240\n",
      "Epoch 164/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1830 - loss: 0.3776 - val_accuracy: 0.1918 - val_loss: 0.1398\n",
      "Epoch 165/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - accuracy: 0.1724 - loss: 0.3776 - val_accuracy: 0.1918 - val_loss: 0.1357\n",
      "Epoch 166/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.1695 - loss: 0.3799 - val_accuracy: 0.1918 - val_loss: 0.1318\n",
      "Epoch 167/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.1779 - loss: 0.3815 - val_accuracy: 0.1918 - val_loss: 0.1384\n",
      "Epoch 168/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.1787 - loss: 0.3795 - val_accuracy: 0.1918 - val_loss: 0.1367\n",
      "Epoch 169/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - accuracy: 0.1763 - loss: 0.3811 - val_accuracy: 0.1918 - val_loss: 0.1390\n",
      "Epoch 170/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.1742 - loss: 0.3823 - val_accuracy: 0.1918 - val_loss: 0.1358\n",
      "Epoch 171/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.1725 - loss: 0.3757 - val_accuracy: 0.1918 - val_loss: 0.1410\n",
      "Epoch 172/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.1788 - loss: 0.3774 - val_accuracy: 0.1918 - val_loss: 0.1403\n",
      "Epoch 173/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.1825 - loss: 0.3776 - val_accuracy: 0.1918 - val_loss: 0.1183\n",
      "Epoch 174/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1828 - loss: 0.3785 - val_accuracy: 0.1918 - val_loss: 0.1397\n",
      "Epoch 175/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.1838 - loss: 0.3789 - val_accuracy: 0.1918 - val_loss: 0.1345\n",
      "Epoch 176/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - accuracy: 0.1779 - loss: 0.3701 - val_accuracy: 0.1918 - val_loss: 0.1336\n",
      "Epoch 177/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.1833 - loss: 0.3699 - val_accuracy: 0.1918 - val_loss: 0.1381\n",
      "Epoch 178/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.1785 - loss: 0.3813 - val_accuracy: 0.1918 - val_loss: 0.1367\n",
      "Epoch 179/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1763 - loss: 0.3767 - val_accuracy: 0.1918 - val_loss: 0.1428\n",
      "Epoch 180/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - accuracy: 0.1750 - loss: 0.3769 - val_accuracy: 0.1918 - val_loss: 0.1364\n",
      "Epoch 181/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - accuracy: 0.1933 - loss: 0.3741 - val_accuracy: 0.1918 - val_loss: 0.1335\n",
      "Epoch 182/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 0.1729 - loss: 0.3724 - val_accuracy: 0.1918 - val_loss: 0.1333\n",
      "Epoch 183/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1758 - loss: 0.3753 - val_accuracy: 0.1918 - val_loss: 0.1295\n",
      "Epoch 184/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.1812 - loss: 0.3711 - val_accuracy: 0.1918 - val_loss: 0.1338\n",
      "Epoch 185/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1824 - loss: 0.3723 - val_accuracy: 0.1918 - val_loss: 0.1199\n",
      "Epoch 186/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.1789 - loss: 0.3768 - val_accuracy: 0.1918 - val_loss: 0.1334\n",
      "Epoch 187/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - accuracy: 0.1863 - loss: 0.3739 - val_accuracy: 0.1918 - val_loss: 0.1264\n",
      "Epoch 188/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.1716 - loss: 0.3709 - val_accuracy: 0.1918 - val_loss: 0.1313\n",
      "Epoch 189/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 0.1785 - loss: 0.3730 - val_accuracy: 0.1918 - val_loss: 0.1298\n",
      "Epoch 190/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - accuracy: 0.1831 - loss: 0.3689 - val_accuracy: 0.1918 - val_loss: 0.1276\n",
      "Epoch 191/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - accuracy: 0.1794 - loss: 0.3667 - val_accuracy: 0.1918 - val_loss: 0.1270\n",
      "Epoch 192/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.1765 - loss: 0.3656 - val_accuracy: 0.1918 - val_loss: 0.1307\n",
      "Epoch 193/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1771 - loss: 0.3705 - val_accuracy: 0.1918 - val_loss: 0.1219\n",
      "Epoch 194/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - accuracy: 0.1789 - loss: 0.3740 - val_accuracy: 0.1918 - val_loss: 0.1325\n",
      "Epoch 195/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.1899 - loss: 0.3724 - val_accuracy: 0.1918 - val_loss: 0.1316\n",
      "Epoch 196/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1728 - loss: 0.3677 - val_accuracy: 0.1918 - val_loss: 0.1331\n",
      "Epoch 197/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 0.1707 - loss: 0.3729 - val_accuracy: 0.1918 - val_loss: 0.1390\n",
      "Epoch 198/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.1948 - loss: 0.3637 - val_accuracy: 0.1918 - val_loss: 0.1396\n",
      "Epoch 199/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - accuracy: 0.1821 - loss: 0.3668 - val_accuracy: 0.1939 - val_loss: 0.1300\n",
      "Epoch 200/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1869 - loss: 0.3601 - val_accuracy: 0.2020 - val_loss: 0.1193\n",
      "Epoch 201/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.1823 - loss: 0.3715 - val_accuracy: 0.1918 - val_loss: 0.1361\n",
      "Epoch 202/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - accuracy: 0.1853 - loss: 0.3711 - val_accuracy: 0.1939 - val_loss: 0.1288\n",
      "Epoch 203/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.1914 - loss: 0.3673 - val_accuracy: 0.2000 - val_loss: 0.1250\n",
      "Epoch 204/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - accuracy: 0.1879 - loss: 0.3695 - val_accuracy: 0.1939 - val_loss: 0.1364\n",
      "Epoch 205/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.1888 - loss: 0.3650 - val_accuracy: 0.1939 - val_loss: 0.1391\n",
      "Epoch 206/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 0.1768 - loss: 0.3682 - val_accuracy: 0.2000 - val_loss: 0.1300\n",
      "Epoch 207/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.1844 - loss: 0.3701 - val_accuracy: 0.1939 - val_loss: 0.1374\n",
      "Epoch 208/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.1870 - loss: 0.3695 - val_accuracy: 0.2000 - val_loss: 0.1282\n",
      "Epoch 209/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.1767 - loss: 0.3734 - val_accuracy: 0.2041 - val_loss: 0.1247\n",
      "Epoch 210/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - accuracy: 0.1964 - loss: 0.3744 - val_accuracy: 0.1939 - val_loss: 0.1355\n",
      "Epoch 211/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - accuracy: 0.1943 - loss: 0.3670 - val_accuracy: 0.2245 - val_loss: 0.1109\n",
      "Epoch 212/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - accuracy: 0.2032 - loss: 0.3637 - val_accuracy: 0.2041 - val_loss: 0.1211\n",
      "Epoch 213/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - accuracy: 0.1908 - loss: 0.3631 - val_accuracy: 0.2020 - val_loss: 0.1254\n",
      "Epoch 214/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.1859 - loss: 0.3727 - val_accuracy: 0.2000 - val_loss: 0.1348\n",
      "Epoch 215/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.1895 - loss: 0.3610 - val_accuracy: 0.1939 - val_loss: 0.1428\n",
      "Epoch 216/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.1921 - loss: 0.3696 - val_accuracy: 0.2020 - val_loss: 0.1274\n",
      "Epoch 217/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.2038 - loss: 0.3623 - val_accuracy: 0.1939 - val_loss: 0.1407\n",
      "Epoch 218/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.2019 - loss: 0.3599 - val_accuracy: 0.2143 - val_loss: 0.1170\n",
      "Epoch 219/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - accuracy: 0.1879 - loss: 0.3698 - val_accuracy: 0.2000 - val_loss: 0.1328\n",
      "Epoch 220/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - accuracy: 0.1913 - loss: 0.3628 - val_accuracy: 0.2000 - val_loss: 0.1320\n",
      "Epoch 221/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1903 - loss: 0.3672 - val_accuracy: 0.2000 - val_loss: 0.1343\n",
      "Epoch 222/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.1830 - loss: 0.3779 - val_accuracy: 0.2082 - val_loss: 0.1214\n",
      "Epoch 223/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.1979 - loss: 0.3652 - val_accuracy: 0.2000 - val_loss: 0.1361\n",
      "Epoch 224/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - accuracy: 0.1862 - loss: 0.3697 - val_accuracy: 0.2020 - val_loss: 0.1276\n",
      "Epoch 225/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.2017 - loss: 0.3557 - val_accuracy: 0.2000 - val_loss: 0.1320\n",
      "Epoch 226/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - accuracy: 0.1945 - loss: 0.3597 - val_accuracy: 0.1939 - val_loss: 0.1427\n",
      "Epoch 227/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.1940 - loss: 0.3585 - val_accuracy: 0.2020 - val_loss: 0.1296\n",
      "Epoch 228/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - accuracy: 0.1830 - loss: 0.3573 - val_accuracy: 0.2041 - val_loss: 0.1263\n",
      "Epoch 229/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1912 - loss: 0.3637 - val_accuracy: 0.2082 - val_loss: 0.1222\n",
      "Epoch 230/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - accuracy: 0.1906 - loss: 0.3579 - val_accuracy: 0.2061 - val_loss: 0.1237\n",
      "Epoch 231/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.2049 - loss: 0.3578 - val_accuracy: 0.2000 - val_loss: 0.1341\n",
      "Epoch 232/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.2089 - loss: 0.3565 - val_accuracy: 0.2000 - val_loss: 0.1388\n",
      "Epoch 233/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - accuracy: 0.1975 - loss: 0.3631 - val_accuracy: 0.2041 - val_loss: 0.1272\n",
      "Epoch 234/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.2016 - loss: 0.3638 - val_accuracy: 0.2000 - val_loss: 0.1339\n",
      "Epoch 235/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2041 - loss: 0.3568 - val_accuracy: 0.2061 - val_loss: 0.1251\n",
      "Epoch 236/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.1949 - loss: 0.3635 - val_accuracy: 0.2061 - val_loss: 0.1233\n",
      "Epoch 237/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - accuracy: 0.1962 - loss: 0.3635 - val_accuracy: 0.2061 - val_loss: 0.1249\n",
      "Epoch 238/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.2010 - loss: 0.3659 - val_accuracy: 0.2020 - val_loss: 0.1313\n",
      "Epoch 239/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - accuracy: 0.1953 - loss: 0.3654 - val_accuracy: 0.2020 - val_loss: 0.1326\n",
      "Epoch 240/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.1910 - loss: 0.3680 - val_accuracy: 0.2000 - val_loss: 0.1382\n",
      "Epoch 241/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.1929 - loss: 0.3613 - val_accuracy: 0.2061 - val_loss: 0.1244\n",
      "Epoch 242/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.1970 - loss: 0.3675 - val_accuracy: 0.2000 - val_loss: 0.1359\n",
      "Epoch 243/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.1905 - loss: 0.3720 - val_accuracy: 0.2061 - val_loss: 0.1250\n",
      "Epoch 244/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.1982 - loss: 0.3697 - val_accuracy: 0.2122 - val_loss: 0.1189\n",
      "Epoch 245/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.2023 - loss: 0.3595 - val_accuracy: 0.2020 - val_loss: 0.1334\n",
      "Epoch 246/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.1993 - loss: 0.3605 - val_accuracy: 0.2061 - val_loss: 0.1247\n",
      "Epoch 247/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2010 - loss: 0.3608 - val_accuracy: 0.2082 - val_loss: 0.1226\n",
      "Epoch 248/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1973 - loss: 0.3527 - val_accuracy: 0.2082 - val_loss: 0.1208\n",
      "Epoch 249/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.1917 - loss: 0.3668 - val_accuracy: 0.2041 - val_loss: 0.1284\n",
      "Epoch 250/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.2013 - loss: 0.3572 - val_accuracy: 0.2143 - val_loss: 0.1184\n",
      "Epoch 251/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.2069 - loss: 0.3689 - val_accuracy: 0.2061 - val_loss: 0.1232\n",
      "Epoch 252/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1967 - loss: 0.3600 - val_accuracy: 0.1939 - val_loss: 0.1430\n",
      "Epoch 253/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.1922 - loss: 0.3674 - val_accuracy: 0.2041 - val_loss: 0.1315\n",
      "Epoch 254/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - accuracy: 0.1932 - loss: 0.3681 - val_accuracy: 0.2020 - val_loss: 0.1333\n",
      "Epoch 255/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1951 - loss: 0.3559 - val_accuracy: 0.1939 - val_loss: 0.1427\n",
      "Epoch 256/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.2107 - loss: 0.3700 - val_accuracy: 0.2020 - val_loss: 0.1267\n",
      "Epoch 257/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - accuracy: 0.1906 - loss: 0.3636 - val_accuracy: 0.2020 - val_loss: 0.1322\n",
      "Epoch 258/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1943 - loss: 0.3598 - val_accuracy: 0.2082 - val_loss: 0.1201\n",
      "Epoch 259/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.1926 - loss: 0.3776 - val_accuracy: 0.2082 - val_loss: 0.1214\n",
      "Epoch 260/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - accuracy: 0.2012 - loss: 0.3610 - val_accuracy: 0.2000 - val_loss: 0.1359\n",
      "Epoch 261/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.1869 - loss: 0.3735 - val_accuracy: 0.2041 - val_loss: 0.1285\n",
      "Epoch 262/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.1988 - loss: 0.3529 - val_accuracy: 0.2041 - val_loss: 0.1260\n",
      "Epoch 263/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.1979 - loss: 0.3585 - val_accuracy: 0.2020 - val_loss: 0.1322\n",
      "Epoch 264/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.2001 - loss: 0.3600 - val_accuracy: 0.2000 - val_loss: 0.1360\n",
      "Epoch 265/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.2023 - loss: 0.3541 - val_accuracy: 0.2082 - val_loss: 0.1216\n",
      "Epoch 266/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - accuracy: 0.1938 - loss: 0.3740 - val_accuracy: 0.2082 - val_loss: 0.1216\n",
      "Epoch 267/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1971 - loss: 0.3540 - val_accuracy: 0.2020 - val_loss: 0.1265\n",
      "Epoch 268/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - accuracy: 0.2075 - loss: 0.3474 - val_accuracy: 0.2082 - val_loss: 0.1233\n",
      "Epoch 269/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.2032 - loss: 0.3705 - val_accuracy: 0.2000 - val_loss: 0.1358\n",
      "Epoch 270/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1946 - loss: 0.3540 - val_accuracy: 0.2061 - val_loss: 0.1234\n",
      "Epoch 271/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1983 - loss: 0.3586 - val_accuracy: 0.2041 - val_loss: 0.1245\n",
      "Epoch 272/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2024 - loss: 0.3561 - val_accuracy: 0.2102 - val_loss: 0.1220\n",
      "Epoch 273/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1950 - loss: 0.3559 - val_accuracy: 0.2204 - val_loss: 0.1187\n",
      "Epoch 274/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step - accuracy: 0.2060 - loss: 0.3652 - val_accuracy: 0.2408 - val_loss: 0.1096\n",
      "Epoch 275/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2071 - loss: 0.3554 - val_accuracy: 0.2000 - val_loss: 0.1376\n",
      "Epoch 276/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - accuracy: 0.1973 - loss: 0.3635 - val_accuracy: 0.2286 - val_loss: 0.1169\n",
      "Epoch 277/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.1926 - loss: 0.3747 - val_accuracy: 0.2061 - val_loss: 0.1230\n",
      "Epoch 278/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.1914 - loss: 0.3633 - val_accuracy: 0.2082 - val_loss: 0.1220\n",
      "Epoch 279/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.2052 - loss: 0.3473 - val_accuracy: 0.2265 - val_loss: 0.1163\n",
      "Epoch 280/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1961 - loss: 0.3680 - val_accuracy: 0.2041 - val_loss: 0.1293\n",
      "Epoch 281/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.1960 - loss: 0.3670 - val_accuracy: 0.2041 - val_loss: 0.1246\n",
      "Epoch 282/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.2005 - loss: 0.3643 - val_accuracy: 0.2041 - val_loss: 0.1243\n",
      "Epoch 283/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2011 - loss: 0.3613 - val_accuracy: 0.2041 - val_loss: 0.1253\n",
      "Epoch 284/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.1938 - loss: 0.3632 - val_accuracy: 0.2061 - val_loss: 0.1234\n",
      "Epoch 285/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2048 - loss: 0.3576 - val_accuracy: 0.2061 - val_loss: 0.1231\n",
      "Epoch 286/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.2001 - loss: 0.3565 - val_accuracy: 0.2286 - val_loss: 0.1164\n",
      "Epoch 287/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - accuracy: 0.2105 - loss: 0.3611 - val_accuracy: 0.2041 - val_loss: 0.1304\n",
      "Epoch 288/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.1983 - loss: 0.3640 - val_accuracy: 0.2184 - val_loss: 0.1208\n",
      "Epoch 289/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - accuracy: 0.2081 - loss: 0.3607 - val_accuracy: 0.2041 - val_loss: 0.1290\n",
      "Epoch 290/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.2053 - loss: 0.3588 - val_accuracy: 0.2265 - val_loss: 0.1174\n",
      "Epoch 291/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2043 - loss: 0.3604 - val_accuracy: 0.2327 - val_loss: 0.1160\n",
      "Epoch 292/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2158 - loss: 0.3467 - val_accuracy: 0.2041 - val_loss: 0.1256\n",
      "Epoch 293/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 0.1996 - loss: 0.3610 - val_accuracy: 0.2041 - val_loss: 0.1251\n",
      "Epoch 294/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - accuracy: 0.2154 - loss: 0.3497 - val_accuracy: 0.2082 - val_loss: 0.1222\n",
      "Epoch 295/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.2058 - loss: 0.3671 - val_accuracy: 0.2041 - val_loss: 0.1359\n",
      "Epoch 296/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - accuracy: 0.2126 - loss: 0.3535 - val_accuracy: 0.2041 - val_loss: 0.1267\n",
      "Epoch 297/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - accuracy: 0.2048 - loss: 0.3610 - val_accuracy: 0.2041 - val_loss: 0.1349\n",
      "Epoch 298/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - accuracy: 0.2016 - loss: 0.3610 - val_accuracy: 0.2041 - val_loss: 0.1296\n",
      "Epoch 299/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2008 - loss: 0.3570 - val_accuracy: 0.2082 - val_loss: 0.1228\n",
      "Epoch 300/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.1939 - loss: 0.3696 - val_accuracy: 0.2041 - val_loss: 0.1257\n",
      "Epoch 301/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - accuracy: 0.1973 - loss: 0.3608 - val_accuracy: 0.2163 - val_loss: 0.1208\n",
      "Epoch 302/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.2050 - loss: 0.3582 - val_accuracy: 0.2041 - val_loss: 0.1336\n",
      "Epoch 303/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1989 - loss: 0.3586 - val_accuracy: 0.2041 - val_loss: 0.1362\n",
      "Epoch 304/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.2038 - loss: 0.3533 - val_accuracy: 0.2020 - val_loss: 0.1272\n",
      "Epoch 305/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - accuracy: 0.2023 - loss: 0.3602 - val_accuracy: 0.2163 - val_loss: 0.1195\n",
      "Epoch 306/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2031 - loss: 0.3591 - val_accuracy: 0.2041 - val_loss: 0.1300\n",
      "Epoch 307/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2010 - loss: 0.3580 - val_accuracy: 0.2041 - val_loss: 0.1368\n",
      "Epoch 308/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.1963 - loss: 0.3538 - val_accuracy: 0.2041 - val_loss: 0.1303\n",
      "Epoch 309/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.2144 - loss: 0.3508 - val_accuracy: 0.2041 - val_loss: 0.1367\n",
      "Epoch 310/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - accuracy: 0.2023 - loss: 0.3686 - val_accuracy: 0.2041 - val_loss: 0.1253\n",
      "Epoch 311/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 0.1974 - loss: 0.3656 - val_accuracy: 0.2041 - val_loss: 0.1270\n",
      "Epoch 312/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.1989 - loss: 0.3561 - val_accuracy: 0.2041 - val_loss: 0.1271\n",
      "Epoch 313/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2069 - loss: 0.3617 - val_accuracy: 0.2102 - val_loss: 0.1227\n",
      "Epoch 314/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - accuracy: 0.2108 - loss: 0.3599 - val_accuracy: 0.2388 - val_loss: 0.1128\n",
      "Epoch 315/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2026 - loss: 0.3589 - val_accuracy: 0.2122 - val_loss: 0.1225\n",
      "Epoch 316/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 0.2011 - loss: 0.3667 - val_accuracy: 0.2041 - val_loss: 0.1278\n",
      "Epoch 317/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - accuracy: 0.1994 - loss: 0.3491 - val_accuracy: 0.2020 - val_loss: 0.1277\n",
      "Epoch 318/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 0.2058 - loss: 0.3537 - val_accuracy: 0.2041 - val_loss: 0.1297\n",
      "Epoch 319/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - accuracy: 0.2032 - loss: 0.3552 - val_accuracy: 0.2367 - val_loss: 0.1149\n",
      "Epoch 320/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - accuracy: 0.2103 - loss: 0.3555 - val_accuracy: 0.2204 - val_loss: 0.1196\n",
      "Epoch 321/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - accuracy: 0.2060 - loss: 0.3614 - val_accuracy: 0.2082 - val_loss: 0.1235\n",
      "Epoch 322/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1970 - loss: 0.3681 - val_accuracy: 0.2306 - val_loss: 0.1171\n",
      "Epoch 323/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2046 - loss: 0.3648 - val_accuracy: 0.2082 - val_loss: 0.1245\n",
      "Epoch 324/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - accuracy: 0.2118 - loss: 0.3546 - val_accuracy: 0.2041 - val_loss: 0.1273\n",
      "Epoch 325/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - accuracy: 0.1954 - loss: 0.3663 - val_accuracy: 0.2224 - val_loss: 0.1188\n",
      "Epoch 326/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.1983 - loss: 0.3617 - val_accuracy: 0.2041 - val_loss: 0.1293\n",
      "Epoch 327/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - accuracy: 0.2007 - loss: 0.3528 - val_accuracy: 0.1959 - val_loss: 0.1421\n",
      "Epoch 328/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - accuracy: 0.2089 - loss: 0.3515 - val_accuracy: 0.2041 - val_loss: 0.1315\n",
      "Epoch 329/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 0.2102 - loss: 0.3584 - val_accuracy: 0.2020 - val_loss: 0.1271\n",
      "Epoch 330/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2103 - loss: 0.3544 - val_accuracy: 0.2061 - val_loss: 0.1257\n",
      "Epoch 331/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.2020 - loss: 0.3625 - val_accuracy: 0.2347 - val_loss: 0.1155\n",
      "Epoch 332/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - accuracy: 0.2164 - loss: 0.3599 - val_accuracy: 0.2041 - val_loss: 0.1269\n",
      "Epoch 333/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2088 - loss: 0.3669 - val_accuracy: 0.2041 - val_loss: 0.1277\n",
      "Epoch 334/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - accuracy: 0.2008 - loss: 0.3705 - val_accuracy: 0.2041 - val_loss: 0.1302\n",
      "Epoch 335/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - accuracy: 0.2090 - loss: 0.3624 - val_accuracy: 0.2000 - val_loss: 0.1382\n",
      "Epoch 336/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - accuracy: 0.2181 - loss: 0.3502 - val_accuracy: 0.2041 - val_loss: 0.1309\n",
      "Epoch 337/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2043 - loss: 0.3531 - val_accuracy: 0.2020 - val_loss: 0.1385\n",
      "Epoch 338/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.1871 - loss: 0.3621 - val_accuracy: 0.2041 - val_loss: 0.1281\n",
      "Epoch 339/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - accuracy: 0.2037 - loss: 0.3600 - val_accuracy: 0.2061 - val_loss: 0.1257\n",
      "Epoch 340/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.2033 - loss: 0.3707 - val_accuracy: 0.2184 - val_loss: 0.1228\n",
      "Epoch 341/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - accuracy: 0.2187 - loss: 0.3477 - val_accuracy: 0.2408 - val_loss: 0.1147\n",
      "Epoch 342/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 0.2067 - loss: 0.3749 - val_accuracy: 0.2265 - val_loss: 0.1193\n",
      "Epoch 343/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2096 - loss: 0.3525 - val_accuracy: 0.2041 - val_loss: 0.1333\n",
      "Epoch 344/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - accuracy: 0.2056 - loss: 0.3590 - val_accuracy: 0.2347 - val_loss: 0.1167\n",
      "Epoch 345/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.2007 - loss: 0.3544 - val_accuracy: 0.2041 - val_loss: 0.1274\n",
      "Epoch 346/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.1998 - loss: 0.3612 - val_accuracy: 0.2041 - val_loss: 0.1280\n",
      "Epoch 347/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 0.2042 - loss: 0.3512 - val_accuracy: 0.2041 - val_loss: 0.1372\n",
      "Epoch 348/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - accuracy: 0.2067 - loss: 0.3576 - val_accuracy: 0.2245 - val_loss: 0.1191\n",
      "Epoch 349/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - accuracy: 0.2098 - loss: 0.3580 - val_accuracy: 0.2122 - val_loss: 0.1241\n",
      "Epoch 350/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - accuracy: 0.2088 - loss: 0.3552 - val_accuracy: 0.2061 - val_loss: 0.1253\n",
      "Epoch 351/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.2025 - loss: 0.3626 - val_accuracy: 0.2449 - val_loss: 0.1136\n",
      "Epoch 352/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.2027 - loss: 0.3828 - val_accuracy: 0.2041 - val_loss: 0.1298\n",
      "Epoch 353/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2025 - loss: 0.3667 - val_accuracy: 0.2204 - val_loss: 0.1210\n",
      "Epoch 354/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - accuracy: 0.2034 - loss: 0.3438 - val_accuracy: 0.2204 - val_loss: 0.1205\n",
      "Epoch 355/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.2177 - loss: 0.3520 - val_accuracy: 0.2102 - val_loss: 0.1254\n",
      "Epoch 356/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.2028 - loss: 0.3711 - val_accuracy: 0.2184 - val_loss: 0.1234\n",
      "Epoch 357/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - accuracy: 0.2103 - loss: 0.3549 - val_accuracy: 0.2061 - val_loss: 0.1267\n",
      "Epoch 358/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.2109 - loss: 0.3455 - val_accuracy: 0.2061 - val_loss: 0.1367\n",
      "Epoch 359/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 0.2076 - loss: 0.3583 - val_accuracy: 0.2061 - val_loss: 0.1266\n",
      "Epoch 360/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.1975 - loss: 0.3694 - val_accuracy: 0.2347 - val_loss: 0.1169\n",
      "Epoch 361/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.2062 - loss: 0.3561 - val_accuracy: 0.2408 - val_loss: 0.1149\n",
      "Epoch 362/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2209 - loss: 0.3511 - val_accuracy: 0.2041 - val_loss: 0.1308\n",
      "Epoch 363/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1979 - loss: 0.3635 - val_accuracy: 0.2061 - val_loss: 0.1260\n",
      "Epoch 364/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.2079 - loss: 0.3563 - val_accuracy: 0.2041 - val_loss: 0.1291\n",
      "Epoch 365/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.2053 - loss: 0.3543 - val_accuracy: 0.2020 - val_loss: 0.1380\n",
      "Epoch 366/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.1972 - loss: 0.3626 - val_accuracy: 0.2286 - val_loss: 0.1176\n",
      "Epoch 367/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - accuracy: 0.2130 - loss: 0.3659 - val_accuracy: 0.2041 - val_loss: 0.1286\n",
      "Epoch 368/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - accuracy: 0.2064 - loss: 0.3552 - val_accuracy: 0.2184 - val_loss: 0.1219\n",
      "Epoch 369/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.2128 - loss: 0.3549 - val_accuracy: 0.2286 - val_loss: 0.1178\n",
      "Epoch 370/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2140 - loss: 0.3557 - val_accuracy: 0.2061 - val_loss: 0.1272\n",
      "Epoch 371/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2024 - loss: 0.3548 - val_accuracy: 0.2408 - val_loss: 0.1144\n",
      "Epoch 372/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.2141 - loss: 0.3571 - val_accuracy: 0.2041 - val_loss: 0.1310\n",
      "Epoch 373/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - accuracy: 0.1933 - loss: 0.3556 - val_accuracy: 0.2163 - val_loss: 0.1238\n",
      "Epoch 374/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - accuracy: 0.2088 - loss: 0.3497 - val_accuracy: 0.2041 - val_loss: 0.1301\n",
      "Epoch 375/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 0.2009 - loss: 0.3670 - val_accuracy: 0.2061 - val_loss: 0.1258\n",
      "Epoch 376/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2144 - loss: 0.3482 - val_accuracy: 0.2204 - val_loss: 0.1206\n",
      "Epoch 377/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.2176 - loss: 0.3609 - val_accuracy: 0.2000 - val_loss: 0.1396\n",
      "Epoch 378/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - accuracy: 0.2086 - loss: 0.3504 - val_accuracy: 0.2041 - val_loss: 0.1321\n",
      "Epoch 379/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.1952 - loss: 0.3627 - val_accuracy: 0.2041 - val_loss: 0.1279\n",
      "Epoch 380/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2018 - loss: 0.3551 - val_accuracy: 0.2061 - val_loss: 0.1268\n",
      "Epoch 381/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - accuracy: 0.2158 - loss: 0.3461 - val_accuracy: 0.2041 - val_loss: 0.1365\n",
      "Epoch 382/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2052 - loss: 0.3504 - val_accuracy: 0.2286 - val_loss: 0.1185\n",
      "Epoch 383/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - accuracy: 0.2112 - loss: 0.3553 - val_accuracy: 0.2184 - val_loss: 0.1220\n",
      "Epoch 384/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.2041 - loss: 0.3548 - val_accuracy: 0.2041 - val_loss: 0.1324\n",
      "Epoch 385/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.2063 - loss: 0.3614 - val_accuracy: 0.2143 - val_loss: 0.1247\n",
      "Epoch 386/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.2004 - loss: 0.3549 - val_accuracy: 0.2061 - val_loss: 0.1266\n",
      "Epoch 387/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - accuracy: 0.2015 - loss: 0.3622 - val_accuracy: 0.2184 - val_loss: 0.1224\n",
      "Epoch 388/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - accuracy: 0.2060 - loss: 0.3489 - val_accuracy: 0.2061 - val_loss: 0.1263\n",
      "Epoch 389/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2072 - loss: 0.3596 - val_accuracy: 0.2041 - val_loss: 0.1274\n",
      "Epoch 390/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - accuracy: 0.2040 - loss: 0.3617 - val_accuracy: 0.2122 - val_loss: 0.1250\n",
      "Epoch 391/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 0.2124 - loss: 0.3606 - val_accuracy: 0.2204 - val_loss: 0.1211\n",
      "Epoch 392/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - accuracy: 0.2116 - loss: 0.3585 - val_accuracy: 0.2061 - val_loss: 0.1277\n",
      "Epoch 393/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - accuracy: 0.2092 - loss: 0.3553 - val_accuracy: 0.2020 - val_loss: 0.1363\n",
      "Epoch 394/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - accuracy: 0.2103 - loss: 0.3563 - val_accuracy: 0.2041 - val_loss: 0.1318\n",
      "Epoch 395/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2037 - loss: 0.3640 - val_accuracy: 0.2041 - val_loss: 0.1284\n",
      "Epoch 396/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.2154 - loss: 0.3584 - val_accuracy: 0.2041 - val_loss: 0.1340\n",
      "Epoch 397/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.2039 - loss: 0.3652 - val_accuracy: 0.2041 - val_loss: 0.1282\n",
      "Epoch 398/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2036 - loss: 0.3677 - val_accuracy: 0.2286 - val_loss: 0.1187\n",
      "Epoch 399/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 0.2082 - loss: 0.3532 - val_accuracy: 0.2163 - val_loss: 0.1241\n",
      "Epoch 400/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - accuracy: 0.2023 - loss: 0.3763 - val_accuracy: 0.2061 - val_loss: 0.1268\n",
      "Epoch 401/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2087 - loss: 0.3550 - val_accuracy: 0.2143 - val_loss: 0.1251\n",
      "Epoch 402/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.2113 - loss: 0.3539 - val_accuracy: 0.2041 - val_loss: 0.1281\n",
      "Epoch 403/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.2089 - loss: 0.3639 - val_accuracy: 0.2061 - val_loss: 0.1267\n",
      "Epoch 404/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.2032 - loss: 0.3593 - val_accuracy: 0.2163 - val_loss: 0.1240\n",
      "Epoch 405/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.2155 - loss: 0.3540 - val_accuracy: 0.2184 - val_loss: 0.1238\n",
      "Epoch 406/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.2117 - loss: 0.3589 - val_accuracy: 0.2204 - val_loss: 0.1218\n",
      "Epoch 407/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2118 - loss: 0.3563 - val_accuracy: 0.2061 - val_loss: 0.1285\n",
      "Epoch 408/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - accuracy: 0.2174 - loss: 0.3573 - val_accuracy: 0.2041 - val_loss: 0.1333\n",
      "Epoch 409/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.2083 - loss: 0.3540 - val_accuracy: 0.2204 - val_loss: 0.1236\n",
      "Epoch 410/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.2102 - loss: 0.3592 - val_accuracy: 0.2204 - val_loss: 0.1235\n",
      "Epoch 411/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.2163 - loss: 0.3512 - val_accuracy: 0.2224 - val_loss: 0.1213\n",
      "Epoch 412/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1974 - loss: 0.3645 - val_accuracy: 0.2143 - val_loss: 0.1241\n",
      "Epoch 413/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - accuracy: 0.2125 - loss: 0.3538 - val_accuracy: 0.2204 - val_loss: 0.1238\n",
      "Epoch 414/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.2218 - loss: 0.3580 - val_accuracy: 0.2306 - val_loss: 0.1183\n",
      "Epoch 415/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - accuracy: 0.2176 - loss: 0.3513 - val_accuracy: 0.2143 - val_loss: 0.1244\n",
      "Epoch 416/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - accuracy: 0.2163 - loss: 0.3485 - val_accuracy: 0.2102 - val_loss: 0.1267\n",
      "Epoch 417/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.2053 - loss: 0.3738 - val_accuracy: 0.2163 - val_loss: 0.1258\n",
      "Epoch 418/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2090 - loss: 0.3683 - val_accuracy: 0.2429 - val_loss: 0.1140\n",
      "Epoch 419/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - accuracy: 0.2053 - loss: 0.3732 - val_accuracy: 0.2306 - val_loss: 0.1195\n",
      "Epoch 420/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.2076 - loss: 0.3666 - val_accuracy: 0.2204 - val_loss: 0.1224\n",
      "Epoch 421/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - accuracy: 0.2085 - loss: 0.3631 - val_accuracy: 0.2347 - val_loss: 0.1186\n",
      "Epoch 422/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - accuracy: 0.2080 - loss: 0.3585 - val_accuracy: 0.2204 - val_loss: 0.1238\n",
      "Epoch 423/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - accuracy: 0.2018 - loss: 0.3624 - val_accuracy: 0.2102 - val_loss: 0.1276\n",
      "Epoch 424/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2141 - loss: 0.3498 - val_accuracy: 0.2020 - val_loss: 0.1359\n",
      "Epoch 425/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - accuracy: 0.2202 - loss: 0.3550 - val_accuracy: 0.2041 - val_loss: 0.1310\n",
      "Epoch 426/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973us/step - accuracy: 0.2121 - loss: 0.3489 - val_accuracy: 0.2245 - val_loss: 0.1213\n",
      "Epoch 427/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2066 - loss: 0.3602 - val_accuracy: 0.2204 - val_loss: 0.1230\n",
      "Epoch 428/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.2014 - loss: 0.3650 - val_accuracy: 0.2204 - val_loss: 0.1242\n",
      "Epoch 429/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2152 - loss: 0.3502 - val_accuracy: 0.2122 - val_loss: 0.1263\n",
      "Epoch 430/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.2176 - loss: 0.3599 - val_accuracy: 0.2102 - val_loss: 0.1272\n",
      "Epoch 431/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.2101 - loss: 0.3531 - val_accuracy: 0.2429 - val_loss: 0.1152\n",
      "Epoch 432/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - accuracy: 0.2201 - loss: 0.3481 - val_accuracy: 0.2061 - val_loss: 0.1287\n",
      "Epoch 433/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.2093 - loss: 0.3644 - val_accuracy: 0.2327 - val_loss: 0.1194\n",
      "Epoch 434/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.2158 - loss: 0.3531 - val_accuracy: 0.2429 - val_loss: 0.1162\n",
      "Epoch 435/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2249 - loss: 0.3516 - val_accuracy: 0.2245 - val_loss: 0.1223\n",
      "Epoch 436/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - accuracy: 0.2096 - loss: 0.3650 - val_accuracy: 0.2245 - val_loss: 0.1211\n",
      "Epoch 437/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - accuracy: 0.2141 - loss: 0.3490 - val_accuracy: 0.2245 - val_loss: 0.1208\n",
      "Epoch 438/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.2093 - loss: 0.3633 - val_accuracy: 0.2429 - val_loss: 0.1152\n",
      "Epoch 439/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - accuracy: 0.2288 - loss: 0.3656 - val_accuracy: 0.2367 - val_loss: 0.1180\n",
      "Epoch 440/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2228 - loss: 0.3488 - val_accuracy: 0.2041 - val_loss: 0.1292\n",
      "Epoch 441/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2151 - loss: 0.3516 - val_accuracy: 0.2347 - val_loss: 0.1194\n",
      "Epoch 442/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - accuracy: 0.2111 - loss: 0.3623 - val_accuracy: 0.2184 - val_loss: 0.1243\n",
      "Epoch 443/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.2104 - loss: 0.3466 - val_accuracy: 0.2184 - val_loss: 0.1240\n",
      "Epoch 444/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.2143 - loss: 0.3533 - val_accuracy: 0.2245 - val_loss: 0.1215\n",
      "Epoch 445/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2110 - loss: 0.3603 - val_accuracy: 0.2204 - val_loss: 0.1234\n",
      "Epoch 446/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step - accuracy: 0.2082 - loss: 0.3541 - val_accuracy: 0.2020 - val_loss: 0.1350\n",
      "Epoch 447/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - accuracy: 0.2021 - loss: 0.3639 - val_accuracy: 0.2429 - val_loss: 0.1172\n",
      "Epoch 448/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.2072 - loss: 0.3626 - val_accuracy: 0.2245 - val_loss: 0.1216\n",
      "Epoch 449/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - accuracy: 0.2140 - loss: 0.3517 - val_accuracy: 0.2204 - val_loss: 0.1240\n",
      "Epoch 450/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.2114 - loss: 0.3513 - val_accuracy: 0.2041 - val_loss: 0.1396\n",
      "Epoch 451/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2123 - loss: 0.3688 - val_accuracy: 0.2245 - val_loss: 0.1218\n",
      "Epoch 452/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.2252 - loss: 0.3551 - val_accuracy: 0.2041 - val_loss: 0.1321\n",
      "Epoch 453/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2081 - loss: 0.3472 - val_accuracy: 0.2429 - val_loss: 0.1142\n",
      "Epoch 454/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2200 - loss: 0.3493 - val_accuracy: 0.1959 - val_loss: 0.1410\n",
      "Epoch 455/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1920 - loss: 0.3643 - val_accuracy: 0.2469 - val_loss: 0.1139\n",
      "Epoch 456/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2213 - loss: 0.3586 - val_accuracy: 0.2429 - val_loss: 0.1164\n",
      "Epoch 457/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - accuracy: 0.2137 - loss: 0.3528 - val_accuracy: 0.2082 - val_loss: 0.1287\n",
      "Epoch 458/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - accuracy: 0.2157 - loss: 0.3579 - val_accuracy: 0.2245 - val_loss: 0.1222\n",
      "Epoch 459/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.2054 - loss: 0.3446 - val_accuracy: 0.2020 - val_loss: 0.1385\n",
      "Epoch 460/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - accuracy: 0.2112 - loss: 0.3556 - val_accuracy: 0.2184 - val_loss: 0.1238\n",
      "Epoch 461/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2197 - loss: 0.3484 - val_accuracy: 0.2143 - val_loss: 0.1261\n",
      "Epoch 462/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.2120 - loss: 0.3529 - val_accuracy: 0.2143 - val_loss: 0.1251\n",
      "Epoch 463/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - accuracy: 0.2121 - loss: 0.3540 - val_accuracy: 0.2184 - val_loss: 0.1241\n",
      "Epoch 464/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.2088 - loss: 0.3540 - val_accuracy: 0.2204 - val_loss: 0.1230\n",
      "Epoch 465/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - accuracy: 0.2042 - loss: 0.3596 - val_accuracy: 0.2102 - val_loss: 0.1280\n",
      "Epoch 466/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2140 - loss: 0.3550 - val_accuracy: 0.2020 - val_loss: 0.1338\n",
      "Epoch 467/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.2147 - loss: 0.3570 - val_accuracy: 0.2286 - val_loss: 0.1216\n",
      "Epoch 468/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - accuracy: 0.2126 - loss: 0.3471 - val_accuracy: 0.2061 - val_loss: 0.1282\n",
      "Epoch 469/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1991 - loss: 0.3659 - val_accuracy: 0.2082 - val_loss: 0.1297\n",
      "Epoch 470/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 0.2168 - loss: 0.3448 - val_accuracy: 0.2143 - val_loss: 0.1268\n",
      "Epoch 471/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2046 - loss: 0.3558 - val_accuracy: 0.2020 - val_loss: 0.1334\n",
      "Epoch 472/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.2054 - loss: 0.3572 - val_accuracy: 0.2020 - val_loss: 0.1317\n",
      "Epoch 473/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.2131 - loss: 0.3513 - val_accuracy: 0.2082 - val_loss: 0.1292\n",
      "Epoch 474/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - accuracy: 0.1985 - loss: 0.3646 - val_accuracy: 0.2184 - val_loss: 0.1243\n",
      "Epoch 475/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - accuracy: 0.2165 - loss: 0.3461 - val_accuracy: 0.2143 - val_loss: 0.1267\n",
      "Epoch 476/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 0.2152 - loss: 0.3524 - val_accuracy: 0.2061 - val_loss: 0.1311\n",
      "Epoch 477/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2101 - loss: 0.3574 - val_accuracy: 0.2388 - val_loss: 0.1181\n",
      "Epoch 478/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - accuracy: 0.2205 - loss: 0.3474 - val_accuracy: 0.2020 - val_loss: 0.1319\n",
      "Epoch 479/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.2156 - loss: 0.3545 - val_accuracy: 0.2122 - val_loss: 0.1278\n",
      "Epoch 480/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - accuracy: 0.2189 - loss: 0.3582 - val_accuracy: 0.2469 - val_loss: 0.1129\n",
      "Epoch 481/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.2152 - loss: 0.3517 - val_accuracy: 0.2082 - val_loss: 0.1295\n",
      "Epoch 482/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2134 - loss: 0.3471 - val_accuracy: 0.2265 - val_loss: 0.1200\n",
      "Epoch 483/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.2179 - loss: 0.3550 - val_accuracy: 0.2265 - val_loss: 0.1213\n",
      "Epoch 484/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.2131 - loss: 0.3549 - val_accuracy: 0.2122 - val_loss: 0.1275\n",
      "Epoch 485/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.2155 - loss: 0.3569 - val_accuracy: 0.2143 - val_loss: 0.1264\n",
      "Epoch 486/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2050 - loss: 0.3605 - val_accuracy: 0.2143 - val_loss: 0.1274\n",
      "Epoch 487/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2133 - loss: 0.3547 - val_accuracy: 0.2388 - val_loss: 0.1190\n",
      "Epoch 488/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.2130 - loss: 0.3528 - val_accuracy: 0.2163 - val_loss: 0.1261\n",
      "Epoch 489/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.2143 - loss: 0.3484 - val_accuracy: 0.2143 - val_loss: 0.1260\n",
      "Epoch 490/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.2196 - loss: 0.3544 - val_accuracy: 0.2020 - val_loss: 0.1321\n",
      "Epoch 491/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.2041 - loss: 0.3647 - val_accuracy: 0.2306 - val_loss: 0.1214\n",
      "Epoch 492/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2079 - loss: 0.3561 - val_accuracy: 0.2367 - val_loss: 0.1206\n",
      "Epoch 493/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.2134 - loss: 0.3618 - val_accuracy: 0.2245 - val_loss: 0.1232\n",
      "Epoch 494/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2035 - loss: 0.3563 - val_accuracy: 0.2102 - val_loss: 0.1294\n",
      "Epoch 495/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.2213 - loss: 0.3527 - val_accuracy: 0.2143 - val_loss: 0.1250\n",
      "Epoch 496/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2118 - loss: 0.3537 - val_accuracy: 0.2061 - val_loss: 0.1297\n",
      "Epoch 497/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.2118 - loss: 0.3593 - val_accuracy: 0.2102 - val_loss: 0.1290\n",
      "Epoch 498/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2077 - loss: 0.3535 - val_accuracy: 0.2306 - val_loss: 0.1212\n",
      "Epoch 499/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - accuracy: 0.2157 - loss: 0.3559 - val_accuracy: 0.2673 - val_loss: 0.1092\n",
      "Epoch 500/500\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - accuracy: 0.2179 - loss: 0.3545 - val_accuracy: 0.2143 - val_loss: 0.1256\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise 3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('preprocessed_winequality_data.csv')\n",
    "\n",
    "# input data\n",
    "X = data.iloc[:, :-10].values.astype(np.float32)\n",
    "\n",
    "# output data\n",
    "y = data.iloc[:, -10:].values.astype(np.float32)\n",
    "\n",
    "# Compute class weights\n",
    "y_labels = np.argmax(y, axis=1)  # Convert one-hot encoding to class labels\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_labels), y=y_labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Apply oversampling to balance classes\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build a model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(23, activation='sigmoid', input_shape=(11,)),\n",
    "    keras.layers.Dense(17, activation='sigmoid'),\n",
    "    keras.layers.Dense(13, activation='sigmoid'),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile a model\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.4),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weight_dict)\n",
    "\n",
    "# Evaluate the data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "conf_matrx = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrx, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Fold 1 MSE: 0.0636\n",
      "Training fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Fold 2 MSE: 0.0671\n",
      "Training fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Fold 3 MSE: 0.0605\n",
      "Training fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Fold 4 MSE: 0.0670\n",
      "Training fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Fold 5 MSE: 0.0696\n",
      "Training fold 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Fold 6 MSE: 0.0606\n",
      "Training fold 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Fold 7 MSE: 0.0683\n",
      "Training fold 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Fold 8 MSE: 0.0660\n",
      "Training fold 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Fold 9 MSE: 0.0607\n",
      "Training fold 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Fold 10 MSE: 0.0676\n",
      "\n",
      "Final Results:\n",
      "MSE values for each fold: [0.06359987, 0.06713602, 0.060517024, 0.06702243, 0.069573365, 0.060612537, 0.06827153, 0.06600944, 0.060737234, 0.06755147]\n",
      "Overall Average MSE: 0.0651\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = pd.read_csv('preprocessed_winequality_data.csv')\n",
    "\n",
    "X = data.iloc[:, :-10].values.astype(np.float32)\n",
    "\n",
    "y = data.iloc[:, -10:].values.astype(np.float32)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "mse_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Training fold {fold+1}...\")\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(23, activation='sigmoid', input_shape=(11,)),\n",
    "        keras.layers.Dense(17, activation='sigmoid'),\n",
    "        keras.layers.Dense(13, activation='sigmoid'),\n",
    "        keras.layers.Dense(10, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.4),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=500, batch_size=32, verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "    print(f\"Fold {fold+1} MSE: {mse:.4f}\")\n",
    "\n",
    "average_mse = np.mean(mse_scores)\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"MSE values for each fold:\", mse_scores)\n",
    "print(f\"Overall Average MSE: {average_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with layers=(20, 15, 10), lr=0.4, epochs=300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "MSE: 0.0682\n",
      "Testing with layers=(30, 20, 15), lr=0.1, epochs=500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "MSE: 0.0682\n",
      "Testing with layers=(20, 15, 10), lr=0.2, epochs=300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE: 0.0682\n",
      "Testing with layers=(20, 15, 10), lr=0.1, epochs=500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "MSE: 0.0682\n",
      "Testing with layers=(20, 15, 10), lr=0.1, epochs=500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "MSE: 0.0682\n",
      "Testing with layers=(20, 15, 10), lr=0.4, epochs=300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE: 0.0683\n",
      "Testing with layers=(25, 18, 12), lr=0.4, epochs=300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE: 0.0682\n",
      "Testing with layers=(25, 18, 12), lr=0.2, epochs=300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "MSE: 0.0682\n",
      "Testing with layers=(20, 15, 10), lr=0.2, epochs=500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE: 0.0682\n",
      "Testing with layers=(25, 18, 12), lr=0.4, epochs=300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE: 0.0681\n",
      "\n",
      "Best Hyperparameters:\n",
      "Hidden Layers: (25, 18, 12)\n",
      "Learning Rate: 0.4\n",
      "Epochs: 300\n",
      "Minimum MSE: 0.0681\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5\n",
    "# Use random search\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "\n",
    "data = pd.read_csv('preprocessed_winequality_data.csv')\n",
    "\n",
    "X = data.iloc[:, :-10].values.astype(np.float32)\n",
    "\n",
    "y = data.iloc[:, -10:].values.astype(np.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "hidden_layer_sizes = [(20, 15, 10), (25, 18, 12), (30, 20, 15)]\n",
    "learning_rates = [0.1, 0.2, 0.4, 0.5]\n",
    "epochs_list = [100, 300, 500]\n",
    "\n",
    "best_params = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "for _ in range(10):\n",
    "    # Choose parameter randomly\n",
    "    hidden_layers = random.choice(hidden_layer_sizes)\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    epochs = random.choice(epochs_list)\n",
    "\n",
    "    print(f\"Testing with layers={hidden_layers}, lr={learning_rate}, epochs={epochs}...\")\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(hidden_layers[0], activation='sigmoid', input_shape=(11,)),\n",
    "        keras.layers.Dense(hidden_layers[1], activation='sigmoid'),\n",
    "        keras.layers.Dense(hidden_layers[2], activation='sigmoid'),\n",
    "        keras.layers.Dense(10, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "\n",
    "    # Update the best parameter\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_params = (hidden_layers, learning_rate, epochs)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"Hidden Layers: {best_params[0]}\")\n",
    "print(f\"Learning Rate: {best_params[1]}\")\n",
    "print(f\"Epochs: {best_params[2]}\")\n",
    "print(f\"Minimum MSE: {best_mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
